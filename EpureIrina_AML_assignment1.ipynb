{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Assignment 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading the Data</h2>\n",
    "<p>This code snippet is the one provedid to us in \"starting_script.py\".</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\irina\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\irina\\anaconda3\\lib\\site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\irina\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\irina\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\irina\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\irina\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DATA_INDEX = 0 # Can be 0,1,2,3,4\n",
    "file = f\"./data/dataset-{DATA_INDEX}.npy\" # this f expression requires Python 3\n",
    "data = np.load(file) # loads the file\n",
    "X, Y = data[:,:-1], data[:,-1] # last column represents targets Y, the other columns the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8. 9. 9. 5. 5. 6. 4. 0. 4. 5. 9. 8. 8. 7. 0. 9. 9. 8. 4. 6. 4. 9. 4. 0.\n",
      " 4. 1. 3. 8. 3. 2. 8. 4. 9. 1. 9. 7. 5. 5. 0. 4. 9. 5. 3. 9. 8. 8. 1. 7.\n",
      " 1. 1. 2. 6. 4. 5. 2. 8. 3. 5. 2. 4. 6. 2. 8. 4. 1. 0. 7. 2. 7. 3. 8. 0.\n",
      " 7. 8. 2. 1. 3. 2. 5. 7. 1. 3. 6. 8. 7. 0. 5. 6. 7. 7. 4. 9. 6. 7. 5. 5.\n",
      " 0. 0. 2. 4. 7. 7. 1. 5. 2. 0. 4. 7. 3. 8. 1. 0. 5. 0. 1. 4. 5. 7. 4. 5.\n",
      " 3. 4. 3. 1. 8. 6. 7. 1. 2. 6. 9. 9. 6. 3. 3. 6. 9. 6. 5. 4. 4. 0. 2. 3.\n",
      " 5. 2. 9. 9. 3. 4. 8. 2. 7. 8. 0. 6. 9. 0. 7. 6. 8. 1. 6. 0. 1. 9. 2. 3.\n",
      " 2. 1. 7. 6. 0. 0. 0. 1. 9. 9. 8. 5. 2. 2. 3. 6. 1. 1. 7. 6. 1. 3. 7. 3.\n",
      " 8. 6. 2. 3. 5. 0. 5. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Instead of changing the DATA_INDEX variable before rerunning this Jupyter notebook for each separate dataset, I have decided to write a function based ont the snipped above, which will automate the process somewhat more.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vector containing the dataset indexes\n",
    "data_indexes = {0, 1, 2, 3, 4}\n",
    "\n",
    "\n",
    "# A function that loads a dataset when given its index\n",
    "def load_data(data_index):\n",
    "    file = f\"./data/dataset-{data_index}.npy\" # this f expression requires Python 3\n",
    "    data = np.load(file) # loads the file\n",
    "    X, Y = data[:,:-1], data[:,-1] # last column represents targets Y, the other columns the input features\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 200\n",
      "1 800\n",
      "2 1500\n",
      "3 3000\n",
      "4 5000\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the size of the datasets\n",
    "for data_index in data_indexes:\n",
    "    X, y = load_data(data_index)\n",
    "    print(data_index, len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Splitting the Dataset into Train, Validation and Test Sets</h2>\n",
    "<p>The training, validation and test sets will contain 70%, 20%, and 10% of the data, respectively.</p>\n",
    "<p>https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_data(X,Y):\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "    # Then, split the train set once again into separate train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=2/9, random_state=0) # 2/9 x 0.9 = 0.2\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Seed</h2>\n",
    "<p>Before starting to properly work on solving this assignment, I will set the random seed to an arbitrary value, in order to ensure the reproducibility of my results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A. Grid Search</h2>\n",
    "The first approach I will use for hyperparameter optimization is Grid Search. First, I will select discrete values for each of the hyperparameters that interest me, then I will iterate through all of their combinations and maintain only the one resulting in the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Defining Discrete Values for the Hyperparameters</h3>\n",
    "Here is an attempt at establishing which hyperparameters I should optimize and what discrete values I should consider for them. The parameters of the sklearn.svm.SVC class are the following:\n",
    "<ul>\n",
    "    <li><b>C</b> - the degree of regularization, values: {0.01, 0.1, 1, 10, 100, 1000}</li>\n",
    "    <li><b>kernel</b> - values: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}</li>\n",
    "    <li><b>degree</b> - only for 'poly' kernel, values: {0, 1, 2, 3, 4, 5, 6}</li>\n",
    "    <li><b>gamma</b> - kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’, values: {'scale', 'auto', 0.1, 1, 10, 100]</li>\n",
    "    <li><b>coef0</b> - only significant in ‘poly’ and ‘sigmoid’, values: {0.01, 0.1, 1, 10, 100, 1000}</li>\n",
    "    <li><b>shrinking - values: {True, False}</b></li>\n",
    "    <li><b>probability - values: {True, False}</b></li>\n",
    "    <li><b>tol</b> - values: {1e-5, 1e-4, 1e-3, 1e-2, 1e-1}</li>\n",
    "    <li><b>cache_size</b> - values: {100, 200, 300, 400, 500}</li>\n",
    "    <li><b>class_weight</b> - will not optimize</li>\n",
    "    <li><b>verbose</b> - should not be optimized</li>\n",
    "    <li><b>max_iter</b> - will not optimize</li>\n",
    "    <li><b>decision_function_shape</b> - should not be optimized</li>\n",
    "    <li><b>break_ties</b> - values: {True, False}</li>\n",
    "    <li><b>random_state</b> - should not be optimized, will choose 10 each time</li>\n",
    "</ul>\n",
    " More details at: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2: Performing and Evaluating Grid Search</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# A not so important function that will help print the results\n",
    "def pretty_print(max_C, max_kernel, max_degree, max_gamma, test_accuracy, data_index):\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print(\"For the dataset with index {0}, the accuracy on the test set is {1}.\".format(data_index, test_accuracy))\n",
    "    print(\"Hyperparameters of the SVC predictor:\")\n",
    "    print(\"C = {0}\".format(max_C))\n",
    "    print(\"kernel = {0}\".format(max_kernel))\n",
    "    \n",
    "    # Degree only matters for 'poly' kernels\n",
    "    if max_kernel == 'poly':\n",
    "        print(\"degree = {0}\".format(max_degree))\n",
    "        \n",
    "    # Gamma only matters for 'poly', 'rbf', and 'sigmoid' kernels\n",
    "    if max_kernel in {'poly', 'rbf', 'sigmoid'}:\n",
    "        print(\"gamma = {0}\".format(max_gamma))\n",
    "        \n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "        \n",
    "def grid_search(data_index):\n",
    "    #Load the dataset\n",
    "    X,Y = load_data(data_index)\n",
    "    #print(\"Data Y \", Y)\n",
    "    \n",
    "    # Split the data into training, validation and test sets\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, Y)\n",
    "    \n",
    "    # Save the highest accuracy in a variable\n",
    "    max_accuracy = 0\n",
    "    #print(\"Max accuracy \", max_accuracy)\n",
    "    \n",
    "    # Iterate through all the parameters' values\n",
    "    for C in [0.01, 0.1, 1, 10, 100, 1000]:\n",
    "        for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "            \n",
    "            if kernel == 'linear':\n",
    "                # Create and fit the model\n",
    "                svc = svm.SVC(C=C, kernel=kernel)\n",
    "                svc.fit(X_train, y_train)\n",
    "\n",
    "                # Evaluate the model's accuracy on the validation set\n",
    "                y_val_pred = svc.predict(X_val)\n",
    "                accuracy = accuracy_score(y_val, y_val_pred)\n",
    "                \n",
    "                if accuracy >= max_accuracy:\n",
    "                    max_accuracy = accuracy\n",
    "                    max_C = C\n",
    "                    max_kernel = kernel\n",
    "                \n",
    "            else:\n",
    "                for gamma in  ['scale', 'auto']:\n",
    "                    if kernel == 'poly':\n",
    "                        for degree in [1, 3, 5, 7, 10, 15, 20]:\n",
    "                            # Create and fit the model\n",
    "                            svc = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma)\n",
    "                            svc.fit(X_train, y_train)\n",
    "\n",
    "                            # Evaluate the model's accuracy on the validation set\n",
    "                            y_val_pred = svc.predict(X_val)\n",
    "                            accuracy = accuracy_score(y_val, y_val_pred)\n",
    "                            \n",
    "                            if accuracy >= max_accuracy:\n",
    "                                max_accuracy = accuracy\n",
    "                                max_C = C\n",
    "                                max_kernel = kernel\n",
    "                                max_gamma = gamma\n",
    "                                max_degree = degree\n",
    "                                \n",
    "                    else:\n",
    "                        # Create and fit the model\n",
    "                        svc = svm.SVC(C=C, kernel=kernel, gamma=gamma)\n",
    "                        svc.fit(X_train, y_train)\n",
    "\n",
    "                        # Evaluate the model's accuracy on the validation set\n",
    "                        y_val_pred = svc.predict(X_val)\n",
    "                        accuracy = accuracy_score(y_val, y_val_pred)\n",
    "                        \n",
    "                        if accuracy >= max_accuracy:\n",
    "                            max_accuracy = accuracy\n",
    "                            max_C = C\n",
    "                            max_kernel = kernel\n",
    "                            max_gamma = gamma\n",
    "    \n",
    "    # At the end, recreate and fit one last time the SVC that had the best accuracy\n",
    "    #svc_best = svm.SVC(C=max_C, kernel=max_kernel, degree=max_degree, gamma=max_gamma)\n",
    "    svc_best = svm.SVC(C=max_C, kernel=max_kernel, degree=max_degree, gamma=max_gamma)\n",
    "    svc_best.fit(X_train, y_train)\n",
    "    \n",
    "    # Now, evaluate the accuracy on the test set\n",
    "    y_test_pred = svc_best.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    pretty_print(max_C, max_kernel, max_degree, max_gamma, test_accuracy, data_index)\n",
    "    \n",
    "    del svc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "For the dataset with index 0, the accuracy on the test set is 0.7.\n",
      "Hyperparameters of the SVC predictor:\n",
      "C = 1000\n",
      "kernel = rbf\n",
      "gamma = scale\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the dataset with index 1, the accuracy on the test set is 0.725.\n",
      "Hyperparameters of the SVC predictor:\n",
      "C = 1000\n",
      "kernel = rbf\n",
      "gamma = scale\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the dataset with index 2, the accuracy on the test set is 0.7066666666666667.\n",
      "Hyperparameters of the SVC predictor:\n",
      "C = 1000\n",
      "kernel = rbf\n",
      "gamma = scale\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the dataset with index 3, the accuracy on the test set is 0.76.\n",
      "Hyperparameters of the SVC predictor:\n",
      "C = 1000\n",
      "kernel = rbf\n",
      "gamma = scale\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the dataset with index 4, the accuracy on the test set is 0.75.\n",
      "Hyperparameters of the SVC predictor:\n",
      "C = 1000\n",
      "kernel = rbf\n",
      "gamma = scale\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Finally, we can see the results\n",
    "for data_index in data_indexes:\n",
    "    grid_search(data_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>B. Random Search</h2>\n",
    "The second approach I will use for hyperparameter optimization is Random Search. First, I will select ranges for each of the hyperparameters that interest me, then I will iterate through all of their combinations and maintain only the one resulting in the highest accuracy. I will maintain the same number of combinations for hyperparameters as for the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(data_index):\n",
    "    #Load the dataset\n",
    "    X,Y = load_data(data_index)\n",
    "    #print(\"Data Y \", Y)\n",
    "    \n",
    "    # Split the data into training, validation and test sets\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, Y)\n",
    "    \n",
    "    # Save the highest accuracy in a variable\n",
    "    max_accuracy = 0 # starts as 0, is updated when a higher accuracy is found\n",
    "    max_C = 0\n",
    "    max_kernel = 0\n",
    "    max_degree = 0\n",
    "    max_gamma = 0\n",
    "    #print(\"Max accuracy \", max_accuracy)\n",
    "    \n",
    "    # Iterate through all the parameters' values\n",
    "    for i in range(102):\n",
    "        C = random.uniform(0.01, 1000)\n",
    "        kernel = random.choice(['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "        if kernel == 'linear':\n",
    "            # Create and fit the model\n",
    "            svc = SVC(C=C, kernel=kernel)\n",
    "            svc.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate the model's accuracy on the validation set\n",
    "            y_val_pred = svc.predict(X_val)\n",
    "            accuracy = accuracy_score(y_val, y_val_pred)\n",
    "                \n",
    "            # If the accuracy is higher than the current maximum,\n",
    "            # then update the maximum accuracy and save the values of \n",
    "            # the hyperparameters that led to it.\n",
    "            if accuracy > max_accuracy:\n",
    "                max_accuracy = accuracy\n",
    "                max_C = C\n",
    "                max_kernel = kernel\n",
    "                \n",
    "            #print(svc, accuracy)\n",
    "                \n",
    "            del svc\n",
    "            \n",
    "        else:\n",
    "            gamma = random.choice(['scale', 'auto'])\n",
    "            if kernel == 'poly':\n",
    "                degree = random.randint(1,20)\n",
    "                # Create and fit the model\n",
    "                svc = SVC(C=C, kernel=kernel, degree=degree, gamma=gamma)\n",
    "                svc.fit(X_train, y_train)\n",
    "\n",
    "                # Evaluate the model's accuracy on the validation set\n",
    "                y_val_pred = svc.predict(X_val)\n",
    "                accuracy = accuracy_score(y_val, y_val_pred)\n",
    "                            \n",
    "                if accuracy > max_accuracy:\n",
    "                    max_accuracy = accuracy\n",
    "                    max_C = C\n",
    "                    max_kernel = kernel\n",
    "                    max_degree = degree\n",
    "                    max_gamma = gamma\n",
    "                    \n",
    "                #print(svc, accuracy)\n",
    "                \n",
    "                del svc\n",
    "                \n",
    "            else:\n",
    "                # Create and fit the model\n",
    "                svc = SVC(C=C, kernel=kernel, gamma=gamma)\n",
    "                svc.fit(X_train, y_train)\n",
    "\n",
    "                # Evaluate the model's accuracy on the validation set\n",
    "                y_val_pred = svc.predict(X_val)\n",
    "                accuracy = accuracy_score(y_val, y_val_pred)\n",
    "                        \n",
    "                if accuracy >= max_accuracy:\n",
    "                    max_accuracy = accuracy\n",
    "                    max_C = C\n",
    "                    max_kernel = kernel\n",
    "                    max_gamma = gamma\n",
    "                    \n",
    "                #print(svc, accuracy)\n",
    "                \n",
    "                del svc\n",
    "    \n",
    "    # At the end, recreate and fit one last time the SVC that had the best accuracy\n",
    "    svc_best = SVC(C=max_C, kernel=max_kernel, degree=max_degree, gamma=max_gamma)\n",
    "    svc_best.fit(X_train, y_train)\n",
    "    \n",
    "    # Now, evaluate the accuracy on the test set\n",
    "    y_test_pred = svc_best.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    pretty_print(max_C, max_kernel, max_degree, max_gamma, test_accuracy, data_index)\n",
    "    \n",
    "    del svc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "For the dataset with index 0, the accuracy on the test set is 0.7.\n",
      "Hyperparameters of the SVC predictor:\n",
      "C = 327.15468430788815\n",
      "kernel = rbf\n",
      "gamma = scale\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the dataset with index 1, the accuracy on the test set is 0.725.\n",
      "Hyperparameters of the SVC predictor:\n",
      "C = 346.29691310600754\n",
      "kernel = rbf\n",
      "gamma = scale\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the dataset with index 2, the accuracy on the test set is 0.96.\n",
      "Hyperparameters of the SVC predictor:\n",
      "C = 447.8930716717264\n",
      "kernel = poly\n",
      "degree = 2\n",
      "gamma = auto\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the dataset with index 3, the accuracy on the test set is 0.9933333333333333.\n",
      "Hyperparameters of the SVC predictor:\n",
      "C = 137.80554778693715\n",
      "kernel = poly\n",
      "degree = 2\n",
      "gamma = auto\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "For the dataset with index 4, the accuracy on the test set is 0.75.\n",
      "Hyperparameters of the SVC predictor:\n",
      "C = 795.4791612089141\n",
      "kernel = rbf\n",
      "gamma = scale\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check out the results\n",
    "for data_index in data_indexes:\n",
    "    random_search(data_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>C. Bayesian Optimization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An unfinished attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n",
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n",
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n",
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n",
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n",
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n",
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n",
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n",
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n",
      "[[-3.30945353 -4.59571161 -0.25663391 ...  4.85446625 -2.54075666\n",
      "   1.55370229]\n",
      " [ 5.72554978 -0.25120372  3.34201656 ...  0.92833202  3.26187419\n",
      "  -0.40702635]\n",
      " [-1.9921959  -1.17021443  1.08655897 ...  3.44695246 -2.52676775\n",
      "   0.68051378]\n",
      " ...\n",
      " [-0.68264084 -2.41163357  0.28513491 ... -3.8254249   2.40653586\n",
      "  -6.50226474]\n",
      " [-4.05556948  1.44448798  2.79663958 ... -0.93270131 -6.47108348\n",
      "  -1.62041453]\n",
      " [ 4.33054323  0.2750504   5.15343234 ...  1.44232185 -0.96185653\n",
      "   1.9774413 ]]\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-ec3a1a754e2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg_x_next\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mx_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg_x_next\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0my_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg_x_next\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mX_known\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "#from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "\n",
    "\n",
    "# Get a random pair of values from X and y\n",
    "def query(X, y):\n",
    "    index = random.randint(0, len(X))\n",
    "    return X[index], y[index]\n",
    "\n",
    "\n",
    "# Aquisition function\n",
    "def expected_improvement(f_star, mu, sigma):\n",
    "    z = (f_star - mu) / sigma\n",
    "    return (f_star - mu) * norm.cdf(z) + sigma * norm.pdf(z)\n",
    "\n",
    "\n",
    "# Decide where to sample next\n",
    "def next_x(X_known, y_known, gpr):\n",
    "    dims = len(X_known[0])\n",
    "    bounds = []\n",
    "    for i in range(dims):\n",
    "        bounds.append((-10, 10))\n",
    "        \n",
    "    gpr.fit(X_known, y_known)\n",
    "    \n",
    "    y_min = 1\n",
    "    arg_x_min = None\n",
    "    \n",
    "    def min_objective(X):\n",
    "        X = X.reshape(-1, dims)\n",
    "        f_star, sigma = gpr.predict(X, return_std=True)\n",
    "        sigma = sigma.reshape(-1, 1)\n",
    "        mu = gpr.predict(X_known)\n",
    "        val = -expected_improvement(f_star, mu, sigma).sum()\n",
    "        #print(val)\n",
    "        return val\n",
    "    \n",
    "    # Search starting from 20 different points, to avoid landing in a local minimum\n",
    "    rand_x = np.random.uniform(-10, 10, size=(10, dims))\n",
    "    for i in rand_x:\n",
    "        result = minimize(min_objective, x0=i, bounds=bounds, method='L-BFGS-B')\n",
    "        \n",
    "        # Find the minimum of the aquisition function\n",
    "        if result.fun <= y_min:\n",
    "            y_min = result.fun\n",
    "            print(X)\n",
    "            arg_x_min = np.where(X == result.x)\n",
    "    \n",
    "    # Return the x vale corresponding to the minimum\n",
    "    return arg_x_min\n",
    "\n",
    "# load dataset\n",
    "X, y = load_data(0)\n",
    "\n",
    "# Create sets of known data points and add two points for now\n",
    "X_known, y_known = [], []\n",
    "xi, yi = query(X, y)\n",
    "X_known.append(xi)\n",
    "y_known.append(yi)\n",
    "xi, yi = query(X, y)\n",
    "X_known.append(xi)\n",
    "y_known.append(yi)\n",
    "\n",
    "gpr = GaussianProcessRegressor()\n",
    "\n",
    "for i in range(10):\n",
    "    gpr.fit(X_known, y_known)\n",
    "    next_x(X_known, y_known, gpr)\n",
    "    print(arg_x_next[0])\n",
    "    x_next = X[arg_x_next]\n",
    "    y_next = y[arg_x_next]\n",
    "    \n",
    "    X_known.append(x_next)\n",
    "    y_known.append(y_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
